{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AetherMindAI Launcher üöÄ\n",
    "\n",
    "Welcome to the AetherMindAI Launcher notebook! This notebook will guide you through cloning the repository, installing dependencies, and launching the AetherMindAI application in either a terminal chat loop or a Gradio-based web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone AetherMindAI Repository\n",
    "\n",
    "This cell clones the AetherMindAI GitHub repository into your Colab environment and navigates into the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    repo_url = \"https://github.com/Divpw/AetherMindAI.git\"\n",
    "    repo_name = \"AetherMindAI\"\n",
    "\n",
    "    if os.path.isdir(repo_name):\n",
    "        print(f\"Repository '{repo_name}' already cloned. Skipping clone.\")\n",
    "    else:\n",
    "        print(f\"Cloning repository from {repo_url}...\")\n",
    "        !git clone {repo_url}\n",
    "        print(\"Repository cloned successfully!\")\n",
    "\n",
    "    # Change directory into the repository\n",
    "    os.chdir(repo_name)\n",
    "    print(f\"Changed current directory to: {os.getcwd()}\")\n",
    "\n",
    "    print(\"\\nRepository contents:\")\n",
    "    !ls -lA\n",
    "\n",
    "    print(\"\\n‚úÖ GitHub Integration cell completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred during GitHub Integration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "This cell installs all the necessary Python libraries listed in the `requirements.txt` file. We use the `--upgrade` flag to ensure the latest compatible versions are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    # Ensure we are in the repository directory\n",
    "    repo_name = \"AetherMindAI\"\n",
    "    if not os.getcwd().endswith(repo_name):\n",
    "        # This might happen if the notebook is run out of order or restarted after cloning\n",
    "        if os.path.isdir(repo_name):\n",
    "            os.chdir(repo_name)\n",
    "            print(f\"Changed directory to {os.getcwd()} to install dependencies.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Repository directory '{repo_name}' not found. Please run the clone step first.\")\n",
    "    \n",
    "    requirements_file = \"requirements.txt\"\n",
    "    if not os.path.exists(requirements_file):\n",
    "        print(f\"‚ùå Error: '{requirements_file}' not found in {os.getcwd()}.\")\n",
    "        print(\"Please ensure the repository was cloned correctly and you are in the repo directory.\")\n",
    "    else:\n",
    "        print(f\"Installing dependencies from {requirements_file}...\")\n",
    "        !pip install -r {requirements_file} --upgrade\n",
    "        print(\"\\n‚úÖ Dependencies installed successfully.\")\n",
    "\n",
    "except FileNotFoundError as fnf_error:\n",
    "    print(f\"‚ùå {fnf_error}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred during dependency installation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Environment\n",
    "\n",
    "This cell performs checks on your Python environment, including the Python version, PyTorch installation, GPU availability, and the installed Gradio version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"--- Environment Checks ---\")\n",
    "try:\n",
    "    # 1. Python Version\n",
    "    print(f\"\\nüêç Python Version:\")\n",
    "    !python --version\n",
    "\n",
    "    # 2. PyTorch and GPU Status\n",
    "    print(f\"\\nüî• PyTorch & GPU Status:\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        print(f\"  CUDA Available: {cuda_available}\")\n",
    "        if cuda_available:\n",
    "            print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"  Current CUDA Device: {torch.cuda.current_device()}\")\n",
    "            print(f\"  Torch Device: cuda\")\n",
    "        else:\n",
    "            print(\"  No GPU detected by PyTorch. Operations will use CPU.\")\n",
    "            print(f\"  Torch Device: cpu\")\n",
    "    except ImportError:\n",
    "        print(\"  ‚ùå PyTorch is not installed. Please check the dependency installation step.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error during PyTorch check: {e}\")\n",
    "\n",
    "    # 3. Gradio Version\n",
    "    print(f\"\\nüé® Gradio Version:\")\n",
    "    try:\n",
    "        # Using importlib to check version as !pip show might be slow or verbose\n",
    "        gradio_spec = importlib.util.find_spec(\"gradio\")\n",
    "        if gradio_spec:\n",
    "            import gradio\n",
    "            print(f\"  Gradio Version: {gradio.__version__}\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Gradio is not installed. Please check the dependency installation step.\")\n",
    "    except ImportError:\n",
    "         print(\"  ‚ùå Gradio is not installed or importable. Please check the dependency installation step.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error during Gradio check: {e}\")\n",
    "        print(\"  Attempting !pip show gradio as fallback...\")\n",
    "        !pip show gradio | grep Version\n",
    "\n",
    "    print(\"\\n‚úÖ Environment checks completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred during environment checks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load AI Modules & Set Up Core Components\n",
    "\n",
    "This cell dynamically loads the core Python modules of AetherMindAI. It initializes the language model, tokenizer, and other tools. Key components like the model, tokenizer, and processing functions are loaded once here to be used by the chosen interface. Flags are set to indicate availability, bypassing fallback logic in the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "print(\"--- Loading AI Modules & Setting Up Core Components ---\")\n",
    "\n",
    "# Ensure we are in the 'AetherMindAI' directory where 'src' is located\n",
    "expected_dir_name = \"AetherMindAI\"\n",
    "if os.path.basename(os.getcwd()) != expected_dir_name:\n",
    "    if os.path.isdir(expected_dir_name):\n",
    "        os.chdir(expected_dir_name)\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        # If 'AetherMindAI' directory doesn't exist at all in current path, error out.\n",
    "        # This case should ideally be caught by the clone step, but as a safeguard:\n",
    "        print(f\"‚ùå Error: Base directory '{expected_dir_name}' not found. Please run the clone step first.\")\n",
    "        # sys.exit() # Or raise an exception if preferred in a notebook context\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = os.path.join(os.getcwd(), \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "print(f\"Added '{src_path}' to sys.path.\")\n",
    "\n",
    "# --- Global variables for loaded components ---\n",
    "notebook_model = None\n",
    "notebook_tokenizer = None\n",
    "notebook_generate_response = None\n",
    "notebook_solve_math_query = None\n",
    "notebook_run_ml_task = None\n",
    "notebook_run_reasoning_prompt = None\n",
    "notebook_aethermind_chat_loop = None # Will be from aethermind_main\n",
    "notebook_launch_gradio_interface = None\n",
    "\n",
    "phi2_available_flag = False\n",
    "math_solver_available_flag = False\n",
    "ml_tools_available_flag = False\n",
    "reasoning_engine_available_flag = False\n",
    "aethermind_main_module_loaded_flag = False\n",
    "gradio_app_module_loaded_flag = False\n",
    "\n",
    "try:\n",
    "    # 1. Load phi2_colab_runner\n",
    "    print(\"\\n1. Loading Phi-2 Model and Tokenizer...\")\n",
    "    try:\n",
    "        import phi2_colab_runner\n",
    "        notebook_model, notebook_tokenizer = phi2_colab_runner.load_model_and_tokenizer()\n",
    "        if notebook_model and notebook_tokenizer:\n",
    "            notebook_generate_response = phi2_colab_runner.generate_response\n",
    "            phi2_available_flag = True\n",
    "            print(\"   ‚úÖ Phi-2 model, tokenizer, and response function loaded successfully.\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Failed to load Phi-2 model or tokenizer. Chat functionality will be impaired.\")\n",
    "    except ImportError:\n",
    "        print(\"   ‚ùå Error: Failed to import 'phi2_colab_runner'. Chat functionality will be unavailable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An unexpected error occurred during Phi-2 initialization: {e}\")\n",
    "\n",
    "    # 2. Load colab_math_solver\n",
    "    print(\"\\n2. Loading Math Solver...\")\n",
    "    try:\n",
    "        import colab_math_solver\n",
    "        notebook_solve_math_query = colab_math_solver.solve_math_query\n",
    "        math_solver_available_flag = True\n",
    "        print(\"   ‚úÖ Math solver loaded successfully.\")\n",
    "    except ImportError:\n",
    "        print(\"   ‚ùå Error: Failed to import 'colab_math_solver'. Math solving functionality will be unavailable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An unexpected error occurred loading math_solver: {e}\")\n",
    "\n",
    "    # 3. Load colab_ml_tools\n",
    "    print(\"\\n3. Loading ML Tools...\")\n",
    "    try:\n",
    "        import colab_ml_tools\n",
    "        notebook_run_ml_task = colab_ml_tools.run_ml_task\n",
    "        ml_tools_available_flag = True\n",
    "        print(\"   ‚úÖ ML tools loaded successfully.\")\n",
    "    except ImportError:\n",
    "        print(\"   ‚ùå Error: Failed to import 'colab_ml_tools'. ML task functionality will be unavailable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An unexpected error occurred loading ml_tools: {e}\")\n",
    "\n",
    "    # 4. Load reasoning_engine\n",
    "    print(\"\\n4. Loading Reasoning Engine...\")\n",
    "    try:\n",
    "        import reasoning_engine\n",
    "        notebook_run_reasoning_prompt = reasoning_engine.run_reasoning_prompt\n",
    "        reasoning_engine_available_flag = True # Added this flag for consistency\n",
    "        print(\"   ‚úÖ Reasoning engine loaded successfully.\")\n",
    "    except ImportError:\n",
    "        print(\"   ‚ùå Error: Failed to import 'reasoning_engine'. Reasoning functionality will be unavailable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An unexpected error occurred loading reasoning_engine: {e}\")\n",
    "\n",
    "    # 5. Load aethermind_main (for chat loop and utility functions)\n",
    "    # We will inject our loaded components into its namespace.\n",
    "    print(\"\\n5. Preparing AetherMind Main Controller...\")\n",
    "    aethermind_main_module = None\n",
    "    try:\n",
    "        spec_main = importlib.util.find_spec(\"aethermind_main\")\n",
    "        if spec_main:\n",
    "            aethermind_main_module = importlib.util.module_from_spec(spec_main)\n",
    "            sys.modules['aethermind_main'] = aethermind_main_module # Add to sys.modules before exec\n",
    "            spec_main.loader.exec_module(aethermind_main_module)\n",
    "\n",
    "            # Inject notebook-loaded components and set flags\n",
    "            aethermind_main_module.phi2_model = notebook_model\n",
    "            aethermind_main_module.phi2_tokenizer = notebook_tokenizer\n",
    "            aethermind_main_module.generate_response = notebook_generate_response # If it uses this name\n",
    "            aethermind_main_module.phi2_available = phi2_available_flag\n",
    "\n",
    "            aethermind_main_module.solve_math_query = notebook_solve_math_query\n",
    "            aethermind_main_module.math_solver_available = math_solver_available_flag\n",
    "\n",
    "            aethermind_main_module.run_ml_task = notebook_run_ml_task\n",
    "            aethermind_main_module.ml_tools_available = ml_tools_available_flag\n",
    "            \n",
    "            # If reasoning_engine functions are used by aethermind_main, inject them too.\n",
    "            # Assuming aethermind_main doesn't directly use reasoning_engine for its core loop.\n",
    "\n",
    "            notebook_aethermind_chat_loop = aethermind_main_module.aethermind_chat_loop\n",
    "            aethermind_main_module_loaded_flag = True\n",
    "            print(\"   ‚úÖ AetherMind main controller prepared and components injected.\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Error: aethermind_main.py spec not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An error occurred preparing aethermind_main: {e}\")\n",
    "\n",
    "    # 6. Load aethermind_gradio_app (for Gradio interface)\n",
    "    # We will inject our loaded components into its namespace.\n",
    "    print(\"\\n6. Preparing AetherMind Gradio App...\")\n",
    "    aethermind_gradio_app_module = None\n",
    "    try:\n",
    "        spec_gradio = importlib.util.find_spec(\"aethermind_gradio_app\")\n",
    "        if spec_gradio:\n",
    "            aethermind_gradio_app_module = importlib.util.module_from_spec(spec_gradio)\n",
    "            sys.modules['aethermind_gradio_app'] = aethermind_gradio_app_module # Add to sys.modules\n",
    "            spec_gradio.loader.exec_module(aethermind_gradio_app_module)\n",
    "\n",
    "            # Inject notebook-loaded components into Gradio app module\n",
    "            aethermind_gradio_app_module.CHAT_MODEL = notebook_model\n",
    "            aethermind_gradio_app_module.CHAT_TOKENIZER = notebook_tokenizer\n",
    "            aethermind_gradio_app_module.CHAT_MODEL_LOADED = phi2_available_flag\n",
    "            # The Gradio app defines its own wrapper, but relies on CHAT_MODEL_LOADED and the globals.\n",
    "            # It also tries to import other tools directly. We need to override those too.\n",
    "            aethermind_gradio_app_module.solve_math_query = notebook_solve_math_query\n",
    "            aethermind_gradio_app_module.run_ml_task = notebook_run_ml_task\n",
    "            aethermind_gradio_app_module.run_reasoning_prompt = notebook_run_reasoning_prompt\n",
    "\n",
    "            notebook_launch_gradio_interface = aethermind_gradio_app_module.launch_gradio_interface\n",
    "            gradio_app_module_loaded_flag = True\n",
    "            print(\"   ‚úÖ AetherMind Gradio app prepared and components injected.\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Error: aethermind_gradio_app.py spec not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An error occurred preparing aethermind_gradio_app: {e}\")\n",
    "\n",
    "    print(\"\\n‚úÖ AI Modules loading and setup process completed.\")\n",
    "    print(\"--- Summary of Loaded Components ---\")\n",
    "    print(f\"Phi-2 Model & Tokenizer: {'Available' if phi2_available_flag else 'Unavailable'}\")\n",
    "    print(f\"Math Solver: {'Available' if math_solver_available_flag else 'Unavailable'}\")\n",
    "    print(f\"ML Tools: {'Available' if ml_tools_available_flag else 'Unavailable'}\")\n",
    "    print(f\"Reasoning Engine: {'Available' if reasoning_engine_available_flag else 'Unavailable'}\")\n",
    "    print(f\"AetherMind Main Controller: {'Prepared' if aethermind_main_module_loaded_flag else 'Not Prepared'}\")\n",
    "    print(f\"AetherMind Gradio App: {'Prepared' if gradio_app_module_loaded_flag else 'Not Prepared'}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå A major error occurred during the module loading and setup process: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Interface and Launch AetherMindAI\n",
    "\n",
    "Use the dropdown menu below to select how you want to interact with AetherMindAI, then run this cell to launch the application.\n",
    "\n",
    "- **Terminal Chat Loop**: A text-based chat interface running directly in the notebook output cell.\n",
    "- **Gradio Interface**: A richer, web-based graphical user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "interface_launcher_cell",
    "outputId": "f29a57f0-599f-4b25-f596-098f74b98974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Launching AetherMindAI Interface ---\n"
     ]
    }
   ],
   "source": [
    "#@title Select Interface and Launch AetherMindAI\n",
    "interface_choice = \"Terminal Chat Loop\" #@param [\"Terminal Chat Loop\", \"Gradio Interface\"]\n",
    "\n",
    "print(\"--- Launching AetherMindAI Interface ---\")\n",
    "try:\n",
    "    if interface_choice == \"Terminal Chat Loop\":\n",
    "        print(\"Selected: Terminal Chat Loop. Initializing...\")\n",
    "        if aethermind_main_module_loaded_flag and callable(notebook_aethermind_chat_loop):\n",
    "            # Ensure the main module is using the correct, notebook-loaded model and tokenizer\n",
    "            if hasattr(aethermind_main_module, 'phi2_model') and hasattr(aethermind_main_module, 'phi2_tokenizer'):\n",
    "                aethermind_main_module.phi2_model = notebook_model\n",
    "                aethermind_main_module.phi2_tokenizer = notebook_tokenizer\n",
    "                aethermind_main_module.generate_response = notebook_generate_response \n",
    "                aethermind_main_module.phi2_available = phi2_available_flag\n",
    "                \n",
    "                aethermind_main_module.solve_math_query = notebook_solve_math_query\n",
    "                aethermind_main_module.math_solver_available = math_solver_available_flag\n",
    "                \n",
    "                aethermind_main_module.run_ml_task = notebook_run_ml_task\n",
    "                aethermind_main_module.ml_tools_available = ml_tools_available_flag\n",
    "                \n",
    "                print(\"Launching Terminal Chat Loop...\")\n",
    "                notebook_aethermind_chat_loop() # Call the function from the prepared module\n",
    "            else:\n",
    "                print(\"‚ùå Error: aethermind_main module is missing expected model/tokenizer attributes.\")\n",
    "        else:\n",
    "            print(\"‚ùå Error: Terminal chat loop function is not available. Please check the module loading step.\")\n",
    "            if not phi2_available_flag:\n",
    "                print(\"   Reason: Core Phi-2 model/tokenizer failed to load.\")\n",
    "\n",
    "    elif interface_choice == \"Gradio Interface\":\n",
    "        print(\"Selected: Gradio Interface. Initializing...\")\n",
    "        if gradio_app_module_loaded_flag and callable(notebook_launch_gradio_interface):\n",
    "            # Ensure the Gradio app module uses the correct, notebook-loaded components\n",
    "            if hasattr(aethermind_gradio_app_module, 'CHAT_MODEL') and \\\n",
    "               hasattr(aethermind_gradio_app_module, 'CHAT_TOKENIZER'):\n",
    "                aethermind_gradio_app_module.CHAT_MODEL = notebook_model\n",
    "                aethermind_gradio_app_module.CHAT_TOKENIZER = notebook_tokenizer\n",
    "                aethermind_gradio_app_module.CHAT_MODEL_LOADED = phi2_available_flag\n",
    "                # Also re-assign the tool functions it might try to import internally\n",
    "                aethermind_gradio_app_module.solve_math_query = notebook_solve_math_query\n",
    "                aethermind_gradio_app_module.run_ml_task = notebook_run_ml_task\n",
    "                aethermind_gradio_app_module.run_reasoning_prompt = notebook_run_reasoning_prompt\n",
    "                \n",
    "                print(\"Launching Gradio Interface... (This may take a moment to start up)\")\n",
    "                notebook_launch_gradio_interface() # Call the function from the prepared module\n",
    "            else:\n",
    "                print(\"‚ùå Error: aethermind_gradio_app module is missing expected model/tokenizer attributes.\")\n",
    "        else:\n",
    "            print(\"‚ùå Error: Gradio interface function is not available. Please check the module loading step.\")\n",
    "            if not phi2_available_flag:\n",
    "                 print(\"   Reason: Core Phi-2 model/tokenizer failed to load, which Gradio UI depends on.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Unknown interface choice '{interface_choice}'.\")\n",
    "\n",
    "except NameError as ne:\n",
    "    print(f\"‚ùå NameError: A required variable or module was not defined: {ne}\")\n",
    "    print(\"   This usually means a previous cell (like Module Loader) did not run successfully or was skipped.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred while launching the interface: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
