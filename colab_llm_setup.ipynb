{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight LLM Setup for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide to set up and run a lightweight open-source language model (like Phi-2 or TinyLlama) from Hugging Face using `transformers` and `accelerate` on a Google Colab free-tier GPU. It also includes the installation of `sympy`, `numpy`, and `scikit-learn` for mathematical and machine learning tasks, while optimizing for minimal RAM usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate einops scipy sympy numpy scikit-learn bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Lightweight Language Model\n",
    "\n",
    "We will use `microsoft/phi-2` as an example. You can replace this with other lightweight models like `TinyLlama/TinyLlama-1.1B-Chat-v1.0`.\n",
    "\n",
    "**Optimizations for RAM:**\n",
    "- `torch_dtype=\"auto\"` or `torch.float16`: Uses half-precision floating points to reduce memory.\n",
    "- `device_map=\"auto\"`: Leverages `accelerate` to automatically distribute model layers across available devices (GPU/CPU), optimizing RAM usage.\n",
    "- `trust_remote_code=True`: Required for some models like Phi-2.\n",
    "- `load_in_8bit=True` or `load_in_4bit=True` (optional): Further reduces memory by quantizing model weights. This requires `bitsandbytes` and might slightly affect performance or accuracy. We'll start without it and it can be added if memory is still an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"microsoft/phi-2\" # Example model, can be changed\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Another option\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=\"auto\", # Use torch.float16 for explicit half-precision\n",
    "        device_map=\"auto\", # Automatically maps model to GPU if available\n",
    "        trust_remote_code=True\n",
    "        # load_in_8bit=True, # Uncomment for 8-bit quantization if needed\n",
    "        # load_in_4bit=True, # Uncomment for 4-bit quantization if needed\n",
    "    )\n",
    "    print(f\"Model '{model_id}' loaded successfully.\")\n",
    "    print(f\"Model device: {model.device}\") # To verify where the model (or parts of it) is loaded\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"If you are seeing an out-of-memory error, try the following:\")\n",
    "    print(\"1. Ensure your Colab runtime has a GPU (Runtime > Change runtime type > T4 GPU).\")\n",
    "    print(\"2. Restart the Colab Kernel (Runtime > Restart session) and try again.\")\n",
    "    print(\"3. Uncomment 'load_in_8bit=True' or 'load_in_4bit=True' in the model loading cell for further RAM optimization (this requires 'bitsandbytes').\")\n",
    "    print(\"4. Try an even smaller model if available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in globals() and 'tokenizer' in globals(): # Check if model and tokenizer were loaded successfully\n",
    "    prompt = \"Write a short story about a curious cat exploring a spaceship.\"\n",
    "    \n",
    "    # Ensure tokenizer has a padding token defined, if not, set it to eos_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True, padding=True).to(model.device) # Move inputs to model's device\n",
    "\n",
    "    # Generate text\n",
    "    try:\n",
    "        print(\"Generating text...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100, # Adjust as needed for longer/shorter responses\n",
    "            # num_beams=5, # Optional: for beam search\n",
    "            # early_stopping=True, # Optional: to stop generation early\n",
    "            # no_repeat_ngram_size=2, # Optional: to prevent repetitive n-grams\n",
    "            eos_token_id=tokenizer.eos_token_id, # Stop generation at EOS token\n",
    "            pad_token_id=tokenizer.pad_token_id # Set pad token id for generation\n",
    "        )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"\\nGenerated Text:\")\n",
    "        print(generated_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "else:\n",
    "    print(\"Model or tokenizer not loaded. Skipping inference example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Math and ML Library Examples\n",
    "\n",
    "This section demonstrates that `sympy`, `numpy`, and `scikit-learn` are installed and can be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Sympy Example ---\")\n",
    "from sympy import symbols, Eq, solve\n",
    "x, y = symbols('x y')\n",
    "eq1 = Eq(x + y, 10)\n",
    "eq2 = Eq(x - y, 4)\n",
    "solution = solve((eq1, eq2), (x, y))\n",
    "print(f\"Solving equations: \\n{eq1}\\n{eq2}\")\n",
    "print(f\"Solution: {solution}\")\n",
    "\n",
    "print(\"\\n--- NumPy Example ---\")\n",
    "import numpy as np\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"NumPy array:\\n{arr}\")\n",
    "print(f\"Sum of all elements: {np.sum(arr)}\")\n",
    "print(f\"Mean of all elements: {np.mean(arr)}\")\n",
    "\n",
    "print(\"\\n--- Scikit-learn Example ---\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_sklearn = LinearRegression()\n",
    "print(f\"Scikit-learn LinearRegression model initialized.\")\n",
    "print(f\"Model parameters: {model_sklearn.get_params()}\")\n",
    "\n",
    "print(\"\\nAll libraries imported and tested successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
